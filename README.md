# ğŸ¤– GenAI RAG Boilerplate with LangChain & Milvus

A production-ready **Retrieval-Augmented Generation (RAG)** boilerplate built with **FastAPI**, **LangChain**, and **Milvus** vector database. This project provides a complete foundation for building AI-powered document search and question-answering systems.

## âœ¨ Key Features

### ğŸ” **Advanced RAG Pipeline**
- **Document Processing** with LangChain loaders (PDF, DOCX, TXT, MD, CSV)
- **Intelligent Chunking** with customizable overlap and size
- **Vector Embeddings** using Sentence Transformers
- **Semantic Search** with similarity scoring
- **Context-Aware Q&A** with multiple AI providers

### ğŸ—„ï¸ **Milvus Vector Database**
- **High-Performance** vector similarity search
- **Scalable Storage** with automatic indexing
- **Collection Management** with schema validation
- **Real-time Operations** with CRUD operations
- **Production Ready** with clustering support

### ğŸš€ **FastAPI Backend**
- **REST API** with OpenAPI documentation
- **Async Operations** for high performance
- **Error Handling** with structured responses
- **File Upload** with validation and processing
- **Health Checks** and monitoring endpoints

### ğŸ¤– **Multi-AI Provider Support**
- **OpenAI** (GPT-3.5, GPT-4, GPT-4 Turbo)
- **Anthropic** (Claude 3 Series)
- **Google** (Gemini Models)
- **Extensible** for custom providers

# ğŸ—ï¸ Architecture Documentation

## ğŸŒ High-Level Design (HLD)

### System Overview
The GenAI RAG Boilerplate is designed as a **microservices-based, AI-powered document processing and retrieval system** that provides intelligent question-answering capabilities through Retrieval-Augmented Generation (RAG).

### Core Architectural Principles
- **Microservices Architecture**: Loosely coupled services with clear boundaries
- **Event-Driven Processing**: Asynchronous document processing and vector operations
- **API-First Design**: RESTful APIs with comprehensive OpenAPI documentation
- **Vector-Native**: Built around high-performance vector similarity search
- **AI-Agnostic**: Pluggable AI provider architecture
- **Container-Ready**: Docker-first deployment with orchestration support

### System Architecture Diagram
```mermaid
graph TB
    subgraph "Client Layer"
        CLI["ğŸ–¥ï¸ CLI Tools\nâ€¢ cURL\nâ€¢ Python SDK\nâ€¢ API Testing"]
        WEB["ğŸŒ Web Interface\nâ€¢ Swagger UI\nâ€¢ Custom Frontend\nâ€¢ Admin Dashboard"]
        SDK["ğŸ“¦ SDK/Libraries\nâ€¢ Python Client\nâ€¢ REST Client\nâ€¢ WebSocket Client"]
    end
    
    subgraph "API Gateway Layer"
        LB["âš–ï¸ Load Balancer\nâ€¢ Nginx/HAProxy\nâ€¢ SSL Termination\nâ€¢ Rate Limiting"]
    end
    
    subgraph "Application Layer"
        API["ğŸš€ FastAPI Backend\nâ€¢ REST Endpoints\nâ€¢ Request Validation\nâ€¢ Error Handling\nâ€¢ Authentication"]
        
        subgraph "Business Logic"
            RAG["ğŸ§  RAG Service\nâ€¢ Document Processing\nâ€¢ LangChain Integration\nâ€¢ Q&A Generation"]
            EMBED["ğŸ”¢ Embedding Service\nâ€¢ Text Vectorization\nâ€¢ Model Management\nâ€¢ Batch Processing"]
            DOC["ğŸ“„ Document Service\nâ€¢ File Processing\nâ€¢ Text Extraction\nâ€¢ Metadata Management"]
        end
    end
    
    subgraph "Data Layer"
        MILVUS[("ğŸ—„ï¸ Milvus VectorDB\nâ€¢ Vector Storage\nâ€¢ Similarity Search\nâ€¢ Index Management\nâ€¢ Collection Schema")]
        MINIO[("ğŸ“¦ MinIO Storage\nâ€¢ Object Storage\nâ€¢ File Persistence\nâ€¢ Backup & Recovery")]
        ETCD[("ğŸ”§ etcd\nâ€¢ Metadata Store\nâ€¢ Configuration\nâ€¢ Service Discovery")]
    end
    
    subgraph "External Services"
        OPENAI["ğŸ¤– OpenAI\nâ€¢ GPT Models\nâ€¢ Text Generation\nâ€¢ Embeddings"]
        ANTHROPIC["ğŸ­ Anthropic\nâ€¢ Claude Models\nâ€¢ Text Analysis"]
        GOOGLE["ğŸ” Google AI\nâ€¢ Gemini Models\nâ€¢ Language Processing"]
        CUSTOM["âš™ï¸ Custom LLMs\nâ€¢ Local Models\nâ€¢ Private APIs"]
    end
    
    subgraph "Infrastructure"
        DOCKER["ğŸ³ Docker\nâ€¢ Containerization\nâ€¢ Service Isolation"]
        K8S["â˜¸ï¸ Kubernetes\nâ€¢ Orchestration\nâ€¢ Scaling\nâ€¢ Health Checks"]
        MONITOR["ğŸ“Š Monitoring\nâ€¢ Prometheus\nâ€¢ Grafana\nâ€¢ Logging"]
    end
    
    CLI --> LB
    WEB --> LB
    SDK --> LB
    LB --> API
    
    API --> RAG
    API --> EMBED
    API --> DOC
    
    RAG --> MILVUS
    RAG --> OPENAI
    RAG --> ANTHROPIC
    RAG --> GOOGLE
    RAG --> CUSTOM
    
    EMBED --> MILVUS
    DOC --> MINIO
    
    MILVUS --> ETCD
    MILVUS --> MINIO
    
    API -.-> MONITOR
    MILVUS -.-> MONITOR
    
    DOCKER -.-> API
    DOCKER -.-> MILVUS
    DOCKER -.-> MINIO
    DOCKER -.-> ETCD
    
    K8S -.-> DOCKER
```

### Component Responsibilities

| Component | Responsibility | Technology |
|-----------|---------------|-----------|
| **FastAPI Backend** | API routing, validation, error handling | FastAPI, Pydantic, Uvicorn |
| **RAG Service** | Document processing, LangChain integration | LangChain, Custom Retrievers |
| **Milvus VectorDB** | Vector storage, similarity search, indexing | Milvus, FAISS, GPU acceleration |
| **MinIO Storage** | Object storage, file persistence, backups | MinIO S3-compatible storage |
| **AI Providers** | Text generation, embeddings, language models | OpenAI, Anthropic, Google AI |

## ğŸ”§ Low-Level Design (LLD)

### Backend Service Architecture

```mermaid
graph TD
    subgraph "FastAPI Application"
        MAIN["ğŸ“± main.py\nâ€¢ App initialization\nâ€¢ Middleware setup\nâ€¢ Exception handlers"]
        
        subgraph "API Layer"
            ROUTES["ğŸ›£ï¸ endpoints.py\nâ€¢ Route definitions\nâ€¢ Request handling\nâ€¢ Response formatting"]
            MODELS["ğŸ“‹ api_models.py\nâ€¢ Pydantic models\nâ€¢ Request validation\nâ€¢ Response schemas"]
        end
        
        subgraph "Business Logic Layer"
            RAG_SVC["ğŸ§  rag_service.py\nâ€¢ Document processing\nâ€¢ LangChain chains\nâ€¢ Q&A generation"]
            
            subgraph "Core Services"
                MILVUS_CLIENT["ğŸ—„ï¸ milvus_client.py\nâ€¢ Vector operations\nâ€¢ Collection management\nâ€¢ Search algorithms"]
                EMBED_SVC["ğŸ”¢ Embedding Service\nâ€¢ Text vectorization\nâ€¢ Model management"]
                DOC_SVC["ğŸ“„ Document Service\nâ€¢ File processing\nâ€¢ Text extraction"]
            end
        end
        
        subgraph "Configuration Layer"
            SETTINGS["âš™ï¸ settings.py\nâ€¢ Environment config\nâ€¢ Pydantic settings\nâ€¢ Validation rules"]
            ENV["ğŸ“ .env\nâ€¢ API keys\nâ€¢ Database URLs\nâ€¢ Feature flags"]
        end
    end
    
    subgraph "External Dependencies"
        LANGCHAIN["ğŸ”— LangChain\nâ€¢ Document loaders\nâ€¢ Text splitters\nâ€¢ Retrievers"]
        SENTENCE_T["ğŸ¯ SentenceTransformers\nâ€¢ Embedding models\nâ€¢ Vector generation"]
        PYMILVUS["ğŸ PyMilvus\nâ€¢ Database client\nâ€¢ Vector operations"]
    end
    
    MAIN --> ROUTES
    MAIN --> SETTINGS
    ROUTES --> MODELS
    ROUTES --> RAG_SVC
    
    RAG_SVC --> MILVUS_CLIENT
    RAG_SVC --> EMBED_SVC
    RAG_SVC --> DOC_SVC
    
    RAG_SVC --> LANGCHAIN
    EMBED_SVC --> SENTENCE_T
    MILVUS_CLIENT --> PYMILVUS
    
    SETTINGS --> ENV
```

### Request Processing Flow

```mermaid
sequenceDiagram
    participant Client
    participant FastAPI
    participant RAGService
    participant MilvusClient
    participant LangChain
    participant AIProvider
    
    Note over Client,AIProvider: Document Upload Flow
    Client->>FastAPI: POST /documents/upload
    FastAPI->>FastAPI: Validate file & request
    FastAPI->>RAGService: process_and_store_document()
    RAGService->>LangChain: Load & split document
    LangChain-->>RAGService: Document chunks
    RAGService->>RAGService: Generate embeddings
    RAGService->>MilvusClient: insert_documents()
    MilvusClient-->>RAGService: Chunk IDs
    RAGService-->>FastAPI: Document ID
    FastAPI-->>Client: Upload response
    
    Note over Client,AIProvider: Question Answering Flow
    Client->>FastAPI: POST /chat/completions
    FastAPI->>FastAPI: Validate question request
    FastAPI->>RAGService: answer_question()
    RAGService->>MilvusClient: search_similar()
    MilvusClient-->>RAGService: Similar chunks
    RAGService->>LangChain: Create retrieval chain
    LangChain->>AIProvider: Generate answer with context
    AIProvider-->>LangChain: AI response
    LangChain-->>RAGService: Formatted answer
    RAGService-->>FastAPI: Answer with sources
    FastAPI-->>Client: Q&A response
```

### Class Architecture

```mermaid
classDiagram
    class Settings {
        +str app_name
        +str secret_key
        +MilvusSettings milvus_settings
        +EmbeddingSettings embedding_settings
        +AIProviders ai_providers
    }
    
    class MilvusClient {
        -SentenceTransformer embedding_model
        -Collection collection
        +connect() async
        +insert_documents() async
        +search_similar() async
        +delete_documents() async
        +get_collection_stats() async
    }
    
    class RAGService {
        -RecursiveCharacterTextSplitter text_splitter
        -Dict[str, LoaderClass] loaders
        -MilvusRetriever retriever
        +process_and_store_document() async
        +search_documents() async
        +answer_question() async
        +create_custom_chain() async
    }
    
    class MilvusRetriever {
        -MilvusClient milvus_client
        -int top_k
        -float score_threshold
        +get_relevant_documents() List[Document]
        +aget_relevant_documents() async List[Document]
    }
    
    class FastAPIApp {
        +FastAPI app
        +include_router()
        +add_middleware()
        +exception_handler()
    }
    
    Settings --* MilvusClient
    Settings --* RAGService
    MilvusClient --* MilvusRetriever
    MilvusRetriever --* RAGService
    RAGService --* FastAPIApp
```

## ğŸ—„ï¸ Data Architecture

### Milvus Collection Schema

```mermaid
erDiagram
    DOCUMENT_CHUNKS {
        varchar id PK "Unique chunk identifier"
        varchar document_id "Original document ID"
        int64 chunk_index "Position within document"
        varchar text "Text content (max 65535 chars)"
        json metadata "Document metadata"
        float_vector embedding "384-dim embedding vector"
    }
    
    COLLECTION_INDEXES {
        string index_name PK
        string field_name
        string index_type "IVF_FLAT, HNSW, etc."
        json index_params "nlist, M, efConstruction"
        string metric_type "L2, IP, COSINE"
    }
    
    COLLECTION_STATS {
        string collection_name PK
        int64 total_entities
        int64 indexed_entities
        timestamp created_at
        timestamp updated_at
        json schema_info
    }
    
    DOCUMENT_CHUNKS ||--o{ COLLECTION_INDEXES : "indexed_by"
    DOCUMENT_CHUNKS ||--|| COLLECTION_STATS : "belongs_to"
```

### Vector Index Strategy

```mermaid
graph LR
    subgraph "Indexing Pipeline"
        TEXT["ğŸ“„ Text Input\nâ€¢ Document content\nâ€¢ Query strings\nâ€¢ Metadata"]
        EMBED["ğŸ”¢ Embedding Model\nâ€¢ SentenceTransformers\nâ€¢ 384 dimensions\nâ€¢ Normalization"]
        VECTOR["ğŸ“Š Vector\nâ€¢ Float array\nâ€¢ L2 normalized\nâ€¢ 384-dim"]
    end
    
    subgraph "Index Types"
        IVF["ğŸ—ï¸ IVF_FLAT\nâ€¢ Inverted File Index\nâ€¢ nlist = 128\nâ€¢ Fast search"]
        HNSW["ğŸ•¸ï¸ HNSW\nâ€¢ Hierarchical NSW\nâ€¢ M = 16\nâ€¢ High accuracy"]
        FLAT["ğŸ“ FLAT\nâ€¢ Brute force\nâ€¢ 100% accuracy\nâ€¢ Small datasets"]
    end
    
    subgraph "Search Strategy"
        QUERY["ğŸ” Query Vector"]
        SEARCH["âš¡ ANN Search\nâ€¢ nprobe = 10\nâ€¢ Top-K results\nâ€¢ Distance metric"]
        RESULTS["ğŸ“‹ Results\nâ€¢ Similarity scores\nâ€¢ Document chunks\nâ€¢ Metadata"]
    end
    
    TEXT --> EMBED
    EMBED --> VECTOR
    VECTOR --> IVF
    VECTOR --> HNSW
    VECTOR --> FLAT
    
    QUERY --> SEARCH
    IVF --> SEARCH
    HNSW --> SEARCH
    FLAT --> SEARCH
    SEARCH --> RESULTS
```

### Data Flow Architecture

```mermaid
flowchart TB
    subgraph "Data Ingestion"
        UPLOAD["ğŸ“¤ File Upload\nâ€¢ PDF, DOCX, TXT\nâ€¢ Size validation\nâ€¢ Type checking"]
        EXTRACT["ğŸ“œ Text Extraction\nâ€¢ PyPDF2\nâ€¢ python-docx\nâ€¢ BeautifulSoup"]
        CHUNK["âœ‚ï¸ Text Chunking\nâ€¢ RecursiveCharacterTextSplitter\nâ€¢ 1000 char chunks\nâ€¢ 200 char overlap"]
    end
    
    subgraph "Vector Processing"
        EMBED["ğŸ”¢ Embedding Generation\nâ€¢ SentenceTransformers\nâ€¢ Batch processing\nâ€¢ Normalization"]
        STORE["ğŸ’¾ Vector Storage\nâ€¢ Milvus collection\nâ€¢ Metadata indexing\nâ€¢ Automatic flushing"]
    end
    
    subgraph "Query Processing"
        QUERY["â“ User Query\nâ€¢ Natural language\nâ€¢ Intent extraction"]
        SEARCH["ğŸ” Similarity Search\nâ€¢ Vector comparison\nâ€¢ Top-K retrieval\nâ€¢ Score filtering"]
        CONTEXT["ğŸ“š Context Assembly\nâ€¢ Relevant chunks\nâ€¢ Metadata enrichment\nâ€¢ Ranking"]
    end
    
    subgraph "Response Generation"
        PROMPT["ğŸ“ Prompt Engineering\nâ€¢ System instructions\nâ€¢ Context injection\nâ€¢ Query formatting"]
        LLM["ğŸ¤– Language Model\nâ€¢ OpenAI GPT\nâ€¢ Anthropic Claude\nâ€¢ Google Gemini"]
        RESPONSE["ğŸ’¬ Structured Response\nâ€¢ Answer text\nâ€¢ Source references\nâ€¢ Confidence scores"]
    end
    
    subgraph "Storage Layer"
        MILVUS_DB[("ğŸ—„ï¸ Milvus\nâ€¢ Vector collections\nâ€¢ Index management\nâ€¢ Query processing")]
        MINIO_DB[("ğŸ“¦ MinIO\nâ€¢ Original files\nâ€¢ Processed documents\nâ€¢ Backup storage")]
        ETCD_DB[("âš™ï¸ etcd\nâ€¢ Metadata\nâ€¢ Configuration\nâ€¢ Service state")]
    end
    
    UPLOAD --> EXTRACT
    EXTRACT --> CHUNK
    CHUNK --> EMBED
    EMBED --> STORE
    STORE --> MILVUS_DB
    
    QUERY --> SEARCH
    SEARCH --> MILVUS_DB
    MILVUS_DB --> CONTEXT
    CONTEXT --> PROMPT
    PROMPT --> LLM
    LLM --> RESPONSE
    
    STORE --> MINIO_DB
    MILVUS_DB --> ETCD_DB
```

### Performance Optimization Strategy

```mermaid
graph TB
    subgraph "Query Optimization"
        CACHE["ğŸš€ Response Caching\nâ€¢ Redis integration\nâ€¢ Query fingerprinting\nâ€¢ TTL management"]
        BATCH["ğŸ“¦ Batch Processing\nâ€¢ Multiple queries\nâ€¢ Embedding batching\nâ€¢ Connection pooling"]
        FILTER["ğŸ¯ Early Filtering\nâ€¢ Metadata filters\nâ€¢ Score thresholds\nâ€¢ Result limiting"]
    end
    
    subgraph "Index Optimization"
        TUNE["âš™ï¸ Index Tuning\nâ€¢ nlist optimization\nâ€¢ nprobe adjustment\nâ€¢ Memory allocation"]
        PART["ğŸ“‚ Collection Partitioning\nâ€¢ Time-based splits\nâ€¢ Category grouping\nâ€¢ Load balancing"]
        COMP["ğŸ—œï¸ Vector Compression\nâ€¢ Quantization\nâ€¢ Dimensionality reduction\nâ€¢ Storage optimization"]
    end
    
    subgraph "System Optimization"
        SCALE["ğŸ“ˆ Horizontal Scaling\nâ€¢ Multiple Milvus nodes\nâ€¢ API replicas\nâ€¢ Load distribution"]
        MONITOR["ğŸ“Š Performance Monitoring\nâ€¢ Query latency\nâ€¢ Throughput metrics\nâ€¢ Resource utilization"]
        AUTO["ğŸ”„ Auto-scaling\nâ€¢ CPU-based scaling\nâ€¢ Memory thresholds\nâ€¢ Queue length triggers"]
    end
    
    CACHE -.-> TUNE
    BATCH -.-> PART
    FILTER -.-> COMP
    
    TUNE --> SCALE
    PART --> MONITOR
    COMP --> AUTO
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **Docker & Docker Compose**
- **Git**

### 1. Clone & Setup

```bash
git clone https://github.com/yourusername/genai-boilerplate-milvus-python.git
cd genai-boilerplate-milvus-python
```

### 2. Environment Configuration

```bash
# Copy environment template
cp backend/.env.example backend/.env

# Edit configuration (add your API keys)
nano backend/.env
```

**Required Configuration:**
```bash
# Security
SECRET_KEY="your-super-secret-key-change-in-production"

# AI Provider (at least one)
OPENAI_API_KEY="sk-your-openai-key"
# ANTHROPIC_API_KEY="claude-your-key"
# GOOGLE_API_KEY="your-google-key"

# Milvus (defaults work with Docker)
MILVUS_HOST="localhost"
MILVUS_PORT=19530
MILVUS_COLLECTION_NAME="documents"

# Embedding Settings
EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2"
EMBEDDING_DIMENSION=384
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

### 3. Docker Deployment

```bash
# Start all services (Milvus + API)
docker-compose up --build -d

# Check service status
docker-compose ps
```

**Services Started:**
- **Milvus Standalone**: `http://localhost:19530` (Vector Database)
- **RAG API**: `http://localhost:8000` (FastAPI Backend)
- **Milvus Admin (Attu)**: `http://localhost:3001` (Optional Web UI)
- **MinIO**: `http://localhost:9001` (Object Storage)

### 4. Verify Installation

```bash
# Health check
curl http://localhost:8000/api/v1/health

# API documentation
open http://localhost:8000/docs
```

### 5. Run Example

```bash
# Install dependencies locally (for examples)
cd backend
pip install -r requirements.txt

# Run the comprehensive example
python examples/rag_example.py

# Test all API endpoints
python examples/api_test.py
```

## ğŸ“š API Documentation

### ğŸ¥ Health & Config

```bash
GET  /api/v1/health          # Service health check
GET  /api/v1/config          # Current configuration
```

### ğŸ“„ Document Management

```bash
POST /api/v1/documents/upload         # Upload & process document
GET  /api/v1/documents/stats          # Collection statistics
DELETE /api/v1/documents/{id}         # Delete document
```

### ğŸ” Search & RAG

```bash
POST /api/v1/documents/search         # Semantic search
POST /api/v1/chat/completions         # RAG question answering
```

### Example Requests

#### Document Upload
```bash
curl -X POST "http://localhost:8000/api/v1/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your-document.pdf"
```

#### Document Search
```bash
curl -X POST "http://localhost:8000/api/v1/documents/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the main benefits of AI?",
    "top_k": 5,
    "score_threshold": 0.1
  }'
```

#### Question Answering
```bash
curl -X POST "http://localhost:8000/api/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How does machine learning work?",
    "top_k": 3,
    "ai_provider": "openai",
    "ai_model": "gpt-3.5-turbo"
  }'
```

## ğŸ”§ Development Setup

### Local Development

```bash
# Backend setup
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Start Milvus only
docker-compose up milvus etcd minio -d

# Run API locally
python src/main.py
```

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `SECRET_KEY` | JWT secret key | Required |
| `MILVUS_HOST` | Milvus server host | `localhost` |
| `MILVUS_PORT` | Milvus server port | `19530` |
| `MILVUS_COLLECTION_NAME` | Collection name | `documents` |
| `EMBEDDING_MODEL` | Sentence transformer model | `all-MiniLM-L6-v2` |
| `EMBEDDING_DIMENSION` | Embedding vector size | `384` |
| `CHUNK_SIZE` | Text chunk size | `1000` |
| `CHUNK_OVERLAP` | Chunk overlap size | `200` |
| `OPENAI_API_KEY` | OpenAI API key | Optional |
| `ANTHROPIC_API_KEY` | Anthropic API key | Optional |
| `GOOGLE_API_KEY` | Google AI API key | Optional |

## ğŸ“– Usage Examples

### Python SDK Usage

```python
import asyncio
from backend.src.services.rag_service import rag_service
from backend.src.core.milvus_client import milvus_client

async def example():
    # Connect to Milvus
    await milvus_client.connect()
    
    # Process a document
    document_id = await rag_service.process_and_store_document(
        file_path="example.pdf",
        metadata={"title": "Example Document"}
    )
    
    # Search for similar content
    results = await rag_service.search_documents(
        query="What is artificial intelligence?",
        top_k=5
    )
    
    # Answer questions using RAG
    response = await rag_service.answer_question(
        question="How does AI work?",
        ai_provider="openai",
        ai_model="gpt-3.5-turbo"
    )
    
    print(f"Answer: {response['answer']}")
    
    # Cleanup
    await milvus_client.disconnect()

# Run example
asyncio.run(example())
```

### Custom LangChain Integration

```python
from backend.src.services.rag_service import rag_service

# Create custom retrieval chain
chain = await rag_service.create_custom_chain(
    chain_type="retrieval_qa",
    ai_provider="openai",
    ai_model="gpt-4"
)

# Use the chain
result = chain({"query": "What are the benefits of RAG?"})
print(result["result"])
```

### Direct Milvus Operations

```python
from backend.src.core.milvus_client import milvus_client

# Connect
await milvus_client.connect()

# Insert documents
documents = [
    {
        "document_id": "doc1",
        "chunk_index": 0,
        "text": "This is example text",
        "metadata": {"source": "example.txt"}
    }
]
chunk_ids = await milvus_client.insert_documents(documents)

# Search similar vectors
results = await milvus_client.search_similar(
    query_text="example query",
    top_k=5,
    score_threshold=0.1
)
```

## ğŸ› ï¸ Advanced Configuration

### Custom Embedding Models

```python
# In your .env file
EMBEDDING_MODEL="sentence-transformers/all-mpnet-base-v2"
EMBEDDING_DIMENSION=768
```

**Supported Models:**
- `all-MiniLM-L6-v2` (384 dimensions) - Fast, lightweight
- `all-mpnet-base-v2` (768 dimensions) - High quality
- `all-MiniLM-L12-v2` (384 dimensions) - Balanced
- Any Sentence-Transformers model

### Milvus Configuration

```yaml
# docker-compose.yml customization
milvus:
  environment:
    - MILVUS_CONFIG_PATH=/milvus/configs/milvus.yaml
  volumes:
    - ./config/milvus.yaml:/milvus/configs/milvus.yaml
```

### AI Provider Configuration

```python
# Custom AI provider integration
from langchain.llms.base import LLM

class CustomLLM(LLM):
    def _call(self, prompt: str, stop=None) -> str:
        # Your custom implementation
        return "Custom response"

# Use in RAG service
rag_service._get_llm = lambda provider, model: CustomLLM()
```

## ğŸ“Š Monitoring & Logging

### Health Checks

```bash
# Application health
curl http://localhost:8000/api/v1/health

# Milvus health
curl http://localhost:9091/healthz

# Collection stats
curl http://localhost:8000/api/v1/documents/stats
```

### Logging Configuration

```python
# Custom logging setup
from loguru import logger

logger.add(
    "logs/rag_{time:YYYY-MM-DD}.log",
    rotation="1 day",
    retention="1 month",
    level="INFO"
)
```

### Performance Monitoring

```bash
# View API metrics
curl http://localhost:8000/metrics

# Monitor Milvus performance
docker logs milvus-standalone --tail 100

# Resource usage
docker stats
```

## ğŸ§ª Testing

### Run Test Suite

```bash
# Unit tests
cd backend
pytest tests/

# API integration tests
python examples/api_test.py

# Load testing
pip install locust
locust -f tests/load_test.py --host=http://localhost:8000
```

### Custom Test Examples

```python
import pytest
from backend.src.services.rag_service import rag_service

@pytest.mark.asyncio
async def test_document_processing():
    document_id = await rag_service.process_and_store_document(
        file_path="test_document.txt"
    )
    assert document_id is not None
    
    # Test search
    results = await rag_service.search_documents(
        query="test query",
        top_k=1
    )
    assert len(results) > 0
```

## ğŸš€ Production Deployment

### Docker Production

```bash
# Production build
docker-compose -f docker-compose.prod.yml up --build -d

# SSL with Nginx
docker-compose -f docker-compose.nginx.yml up -d
```

### Kubernetes Deployment

```yaml
# k8s/milvus-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus
spec:
  replicas: 3
  selector:
    matchLabels:
      app: milvus
  template:
    spec:
      containers:
      - name: milvus
        image: milvusdb/milvus:v2.3.4
        ports:
        - containerPort: 19530
```

### Scaling Considerations

- **Milvus Clustering**: Use multiple Milvus instances
- **API Scaling**: Deploy multiple FastAPI replicas
- **Load Balancing**: Use Nginx or cloud load balancers
- **Database Sharding**: Distribute collections across nodes
- **Caching**: Implement Redis for frequent queries

## ğŸ”’ Security

### API Security

```python
# Add authentication middleware
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer

security = HTTPBearer()

async def verify_token(token: str = Depends(security)):
    # Implement JWT verification
    if not verify_jwt_token(token.credentials):
        raise HTTPException(status_code=401)
    return token
```

### Network Security

```yaml
# docker-compose with network isolation
networks:
  rag-network:
    driver: bridge
    internal: true
  
services:
  milvus:
    networks:
      - rag-network
```

## ğŸ¤ Contributing

### Development Workflow

```bash
# Fork and clone
git clone https://github.com/yourusername/genai-boilerplate-milvus-python.git
cd genai-boilerplate-milvus-python

# Create feature branch
git checkout -b feature/your-feature-name

# Make changes and test
python examples/rag_example.py
python examples/api_test.py

# Submit PR
git commit -am "Add your feature"
git push origin feature/your-feature-name
```

### Code Standards

- **Python**: Black formatting, type hints, docstrings
- **API**: OpenAPI/Swagger documentation
- **Testing**: Pytest with async support
- **Logging**: Structured logging with loguru
- **Error Handling**: Consistent error responses

## ğŸ“„ File Structure

```
genai-boilerplate-milvus-python/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ endpoints.py          # FastAPI routes
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â””â”€â”€ milvus_client.py      # Vector database client
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ api_models.py         # Pydantic models
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ rag_service.py        # LangChain RAG logic
â”‚   â”‚   â””â”€â”€ main.py                   # FastAPI application
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py               # Configuration management
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”œâ”€â”€ rag_example.py            # Usage examples
â”‚   â”‚   â””â”€â”€ api_test.py               # API testing
â”‚   â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile                    # Container build
â”‚   â””â”€â”€ .env.example                  # Environment template
â”œâ”€â”€ docker-compose.yml                # Development setup
â”œâ”€â”€ docker-compose.prod.yml           # Production setup
â””â”€â”€ README.md                         # This file
```

## ğŸ¯ Roadmap

### Current Features
- âœ… FastAPI REST API
- âœ… Milvus vector database integration
- âœ… LangChain RAG pipeline
- âœ… Multi-format document processing
- âœ… Multiple AI provider support
- âœ… Docker containerization

### Planned Features
- ğŸ”„ WebSocket real-time streaming
- ğŸ”„ Advanced chunking strategies
- ğŸ”„ Multi-modal embeddings (text + images)
- ğŸ”„ GraphQL API support
- ğŸ”„ Kubernetes deployment manifests
- ğŸ”„ Monitoring dashboard
- ğŸ”„ Advanced caching layer
- ğŸ”„ Multi-tenant architecture

## ğŸ› Troubleshooting

### Common Issues

#### Milvus Connection Failed
```bash
# Check if Milvus is running
docker-compose ps milvus

# View Milvus logs
docker-compose logs milvus

# Restart Milvus
docker-compose restart milvus
```

#### Embedding Model Download Issues
```bash
# Pre-download models
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"

# Use different model
export EMBEDDING_MODEL="sentence-transformers/all-distilroberta-v1"
```

#### Memory Issues
```bash
# Increase Docker memory limit
# Docker Desktop â†’ Settings â†’ Resources â†’ Memory â†’ 8GB+

# Monitor memory usage
docker stats
```

## ğŸ“ Support

- **ğŸ› Issues**: [GitHub Issues](https://github.com/yourusername/genai-boilerplate-milvus-python/issues)
- **ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/yourusername/genai-boilerplate-milvus-python/discussions)
- **ğŸ“– Documentation**: [Wiki](https://github.com/yourusername/genai-boilerplate-milvus-python/wiki)
- **ğŸš€ Releases**: [Release Notes](https://github.com/yourusername/genai-boilerplate-milvus-python/releases)

## ğŸ“„ License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **[LangChain](https://langchain.com)** - RAG framework and document processing
- **[Milvus](https://milvus.io)** - High-performance vector database
- **[FastAPI](https://fastapi.tiangolo.com)** - Modern Python web framework
- **[Sentence Transformers](https://www.sbert.net)** - Embedding models

---

**ğŸš€ Ready to build powerful RAG applications with cutting-edge vector search!**

*Built with â¤ï¸ for the AI community*
