# ü§ñ GenAI RAG Boilerplate with LangChain & Milvus

A production-ready **Retrieval-Augmented Generation (RAG)** boilerplate built with **FastAPI**, **LangChain**, and **Milvus** vector database. This project provides a complete foundation for building AI-powered document search and question-answering systems.

## ‚ú® Key Features

### üîç **Advanced RAG Pipeline**
- **Document Processing** with LangChain loaders (PDF, DOCX, TXT, MD, CSV)
- **Intelligent Chunking** with customizable overlap and size
- **Vector Embeddings** using Sentence Transformers
- **Semantic Search** with similarity scoring
- **Context-Aware Q&A** with multiple AI providers

### üóÑÔ∏è **Milvus Vector Database**
- **High-Performance** vector similarity search
- **Scalable Storage** with automatic indexing
- **Collection Management** with schema validation
- **Real-time Operations** with CRUD operations
- **Production Ready** with clustering support

### üöÄ **FastAPI Backend**
- **REST API** with OpenAPI documentation
- **Async Operations** for high performance
- **Error Handling** with structured responses
- **File Upload** with validation and processing
- **Health Checks** and monitoring endpoints

### ü§ñ **Multi-AI Provider Support**
- **OpenAI** (GPT-3.5, GPT-4, GPT-4 Turbo)
- **Anthropic** (Claude 3 Series)
- **Google** (Gemini Models)
- **Extensible** for custom providers

# üèóÔ∏è Architecture Documentation

## üåê High-Level Design (HLD)

### System Overview
The GenAI RAG Boilerplate is designed as a **microservices-based, AI-powered document processing and retrieval system** that provides intelligent question-answering capabilities through Retrieval-Augmented Generation (RAG).

### Core Architectural Principles
- **Microservices Architecture**: Loosely coupled services with clear boundaries
- **Event-Driven Processing**: Asynchronous document processing and vector operations
- **API-First Design**: RESTful APIs with comprehensive OpenAPI documentation
- **Vector-Native**: Built around high-performance vector similarity search
- **AI-Agnostic**: Pluggable AI provider architecture
- **Container-Ready**: Docker-first deployment with orchestration support

### System Architecture Diagram
```mermaid
graph TB
    subgraph "Client Layer"
        CLI["üñ•Ô∏è CLI Tools\n‚Ä¢ cURL\n‚Ä¢ Python SDK\n‚Ä¢ API Testing"]
        WEB["üåê Web Interface\n‚Ä¢ Swagger UI\n‚Ä¢ Custom Frontend\n‚Ä¢ Admin Dashboard"]
        SDK["üì¶ SDK/Libraries\n‚Ä¢ Python Client\n‚Ä¢ REST Client\n‚Ä¢ WebSocket Client"]
    end
    
    subgraph "API Gateway Layer"
        LB["‚öñÔ∏è Load Balancer\n‚Ä¢ Nginx/HAProxy\n‚Ä¢ SSL Termination\n‚Ä¢ Rate Limiting"]
    end
    
    subgraph "Application Layer"
        API["üöÄ FastAPI Backend\n‚Ä¢ REST Endpoints\n‚Ä¢ Request Validation\n‚Ä¢ Error Handling\n‚Ä¢ Authentication"]
        
        subgraph "Business Logic"
            RAG["üß† RAG Service\n‚Ä¢ Document Processing\n‚Ä¢ LangChain Integration\n‚Ä¢ Q&A Generation"]
            EMBED["üî¢ Embedding Service\n‚Ä¢ Text Vectorization\n‚Ä¢ Model Management\n‚Ä¢ Batch Processing"]
            DOC["üìÑ Document Service\n‚Ä¢ File Processing\n‚Ä¢ Text Extraction\n‚Ä¢ Metadata Management"]
        end
    end
    
    subgraph "Data Layer"
        MILVUS[("üóÑÔ∏è Milvus VectorDB\n‚Ä¢ Vector Storage\n‚Ä¢ Similarity Search\n‚Ä¢ Index Management\n‚Ä¢ Collection Schema")]
        MINIO[("üì¶ MinIO Storage\n‚Ä¢ Object Storage\n‚Ä¢ File Persistence\n‚Ä¢ Backup & Recovery")]
        ETCD[("üîß etcd\n‚Ä¢ Metadata Store\n‚Ä¢ Configuration\n‚Ä¢ Service Discovery")]
    end
    
    subgraph "External Services"
        OPENAI["ü§ñ OpenAI\n‚Ä¢ GPT Models\n‚Ä¢ Text Generation\n‚Ä¢ Embeddings"]
        ANTHROPIC["üé≠ Anthropic\n‚Ä¢ Claude Models\n‚Ä¢ Text Analysis"]
        GOOGLE["üîç Google AI\n‚Ä¢ Gemini Models\n‚Ä¢ Language Processing"]
        CUSTOM["‚öôÔ∏è Custom LLMs\n‚Ä¢ Local Models\n‚Ä¢ Private APIs"]
    end
    
    subgraph "Infrastructure"
        DOCKER["üê≥ Docker\n‚Ä¢ Containerization\n‚Ä¢ Service Isolation"]
        K8S["‚ò∏Ô∏è Kubernetes\n‚Ä¢ Orchestration\n‚Ä¢ Scaling\n‚Ä¢ Health Checks"]
        MONITOR["üìä Monitoring\n‚Ä¢ Prometheus\n‚Ä¢ Grafana\n‚Ä¢ Logging"]
    end
    
    CLI --> LB
    WEB --> LB
    SDK --> LB
    LB --> API
    
    API --> RAG
    API --> EMBED
    API --> DOC
    
    RAG --> MILVUS
    RAG --> OPENAI
    RAG --> ANTHROPIC
    RAG --> GOOGLE
    RAG --> CUSTOM
    
    EMBED --> MILVUS
    DOC --> MINIO
    
    MILVUS --> ETCD
    MILVUS --> MINIO
    
    API -.-> MONITOR
    MILVUS -.-> MONITOR
    
    DOCKER -.-> API
    DOCKER -.-> MILVUS
    DOCKER -.-> MINIO
    DOCKER -.-> ETCD
    
    K8S -.-> DOCKER
```

### Component Responsibilities

| Component | Responsibility | Technology |
|-----------|---------------|-----------|
| **FastAPI Backend** | API routing, validation, error handling | FastAPI, Pydantic, Uvicorn |
| **RAG Service** | Document processing, LangChain integration | LangChain, Custom Retrievers |
| **Milvus VectorDB** | Vector storage, similarity search, indexing | Milvus, FAISS, GPU acceleration |
| **MinIO Storage** | Object storage, file persistence, backups | MinIO S3-compatible storage |
| **AI Providers** | Text generation, embeddings, language models | OpenAI, Anthropic, Google AI |

## üîß Low-Level Design (LLD)

### Backend Service Architecture

```mermaid
graph TD
    subgraph "FastAPI Application"
        MAIN["üì± main.py\n‚Ä¢ App initialization\n‚Ä¢ Middleware setup\n‚Ä¢ Exception handlers"]
        
        subgraph "API Layer"
            ROUTES["üõ£Ô∏è endpoints.py\n‚Ä¢ Route definitions\n‚Ä¢ Request handling\n‚Ä¢ Response formatting"]
            MODELS["üìã api_models.py\n‚Ä¢ Pydantic models\n‚Ä¢ Request validation\n‚Ä¢ Response schemas"]
        end
        
        subgraph "Business Logic Layer"
            RAG_SVC["üß† rag_service.py\n‚Ä¢ Document processing\n‚Ä¢ LangChain chains\n‚Ä¢ Q&A generation"]
            
            subgraph "Core Services"
                MILVUS_CLIENT["üóÑÔ∏è milvus_client.py\n‚Ä¢ Vector operations\n‚Ä¢ Collection management\n‚Ä¢ Search algorithms"]
                EMBED_SVC["üî¢ Embedding Service\n‚Ä¢ Text vectorization\n‚Ä¢ Model management"]
                DOC_SVC["üìÑ Document Service\n‚Ä¢ File processing\n‚Ä¢ Text extraction"]
            end
        end
        
        subgraph "Configuration Layer"
            SETTINGS["‚öôÔ∏è settings.py\n‚Ä¢ Environment config\n‚Ä¢ Pydantic settings\n‚Ä¢ Validation rules"]
            ENV["üìù .env\n‚Ä¢ API keys\n‚Ä¢ Database URLs\n‚Ä¢ Feature flags"]
        end
    end
    
    subgraph "External Dependencies"
        LANGCHAIN["üîó LangChain\n‚Ä¢ Document loaders\n‚Ä¢ Text splitters\n‚Ä¢ Retrievers"]
        SENTENCE_T["üéØ SentenceTransformers\n‚Ä¢ Embedding models\n‚Ä¢ Vector generation"]
        PYMILVUS["üêç PyMilvus\n‚Ä¢ Database client\n‚Ä¢ Vector operations"]
    end
    
    MAIN --> ROUTES
    MAIN --> SETTINGS
    ROUTES --> MODELS
    ROUTES --> RAG_SVC
    
    RAG_SVC --> MILVUS_CLIENT
    RAG_SVC --> EMBED_SVC
    RAG_SVC --> DOC_SVC
    
    RAG_SVC --> LANGCHAIN
    EMBED_SVC --> SENTENCE_T
    MILVUS_CLIENT --> PYMILVUS
    
    SETTINGS --> ENV
```

### Request Processing Flow

```mermaid
sequenceDiagram
    participant Client
    participant FastAPI
    participant RAGService
    participant MilvusClient
    participant LangChain
    participant AIProvider
    
    Note over Client,AIProvider: Document Upload Flow
    Client->>FastAPI: POST /documents/upload
    FastAPI->>FastAPI: Validate file & request
    FastAPI->>RAGService: process_and_store_document()
    RAGService->>LangChain: Load & split document
    LangChain-->>RAGService: Document chunks
    RAGService->>RAGService: Generate embeddings
    RAGService->>MilvusClient: insert_documents()
    MilvusClient-->>RAGService: Chunk IDs
    RAGService-->>FastAPI: Document ID
    FastAPI-->>Client: Upload response
    
    Note over Client,AIProvider: Question Answering Flow
    Client->>FastAPI: POST /chat/completions
    FastAPI->>FastAPI: Validate question request
    FastAPI->>RAGService: answer_question()
    RAGService->>MilvusClient: search_similar()
    MilvusClient-->>RAGService: Similar chunks
    RAGService->>LangChain: Create retrieval chain
    LangChain->>AIProvider: Generate answer with context
    AIProvider-->>LangChain: AI response
    LangChain-->>RAGService: Formatted answer
    RAGService-->>FastAPI: Answer with sources
    FastAPI-->>Client: Q&A response
```

### Class Architecture

```mermaid
classDiagram
    class Settings {
        +str app_name
        +str secret_key
        +MilvusSettings milvus_settings
        +EmbeddingSettings embedding_settings
        +AIProviders ai_providers
    }
    
    class MilvusClient {
        -SentenceTransformer embedding_model
        -Collection collection
        +connect() async
        +insert_documents() async
        +search_similar() async
        +delete_documents() async
        +get_collection_stats() async
    }
    
    class RAGService {
        -RecursiveCharacterTextSplitter text_splitter
        -Dict[str, LoaderClass] loaders
        -MilvusRetriever retriever
        +process_and_store_document() async
        +search_documents() async
        +answer_question() async
        +create_custom_chain() async
    }
    
    class MilvusRetriever {
        -MilvusClient milvus_client
        -int top_k
        -float score_threshold
        +get_relevant_documents() List[Document]
        +aget_relevant_documents() async List[Document]
    }
    
    class FastAPIApp {
        +FastAPI app
        +include_router()
        +add_middleware()
        +exception_handler()
    }
    
    Settings --* MilvusClient
    Settings --* RAGService
    MilvusClient --* MilvusRetriever
    MilvusRetriever --* RAGService
    RAGService --* FastAPIApp
```

## üóÑÔ∏è Data Architecture

### Milvus Collection Schema

```mermaid
erDiagram
    DOCUMENT_CHUNKS {
        varchar id PK "Unique chunk identifier"
        varchar document_id "Original document ID"
        int64 chunk_index "Position within document"
        varchar text "Text content (max 65535 chars)"
        json metadata "Document metadata"
        float_vector embedding "384-dim embedding vector"
    }
    
    COLLECTION_INDEXES {
        string index_name PK
        string field_name
        string index_type "IVF_FLAT, HNSW, etc."
        json index_params "nlist, M, efConstruction"
        string metric_type "L2, IP, COSINE"
    }
    
    COLLECTION_STATS {
        string collection_name PK
        int64 total_entities
        int64 indexed_entities
        timestamp created_at
        timestamp updated_at
        json schema_info
    }
    
    DOCUMENT_CHUNKS ||--o{ COLLECTION_INDEXES : "indexed_by"
    DOCUMENT_CHUNKS ||--|| COLLECTION_STATS : "belongs_to"
```

### Vector Index Strategy

```mermaid
graph LR
    subgraph "Indexing Pipeline"
        TEXT["üìÑ Text Input\n‚Ä¢ Document content\n‚Ä¢ Query strings\n‚Ä¢ Metadata"]
        EMBED["üî¢ Embedding Model\n‚Ä¢ SentenceTransformers\n‚Ä¢ 384 dimensions\n‚Ä¢ Normalization"]
        VECTOR["üìä Vector\n‚Ä¢ Float array\n‚Ä¢ L2 normalized\n‚Ä¢ 384-dim"]
    end
    
    subgraph "Index Types"
        IVF["üèóÔ∏è IVF_FLAT\n‚Ä¢ Inverted File Index\n‚Ä¢ nlist = 128\n‚Ä¢ Fast search"]
        HNSW["üï∏Ô∏è HNSW\n‚Ä¢ Hierarchical NSW\n‚Ä¢ M = 16\n‚Ä¢ High accuracy"]
        FLAT["üìè FLAT\n‚Ä¢ Brute force\n‚Ä¢ 100% accuracy\n‚Ä¢ Small datasets"]
    end
    
    subgraph "Search Strategy"
        QUERY["üîç Query Vector"]
        SEARCH["‚ö° ANN Search\n‚Ä¢ nprobe = 10\n‚Ä¢ Top-K results\n‚Ä¢ Distance metric"]
        RESULTS["üìã Results\n‚Ä¢ Similarity scores\n‚Ä¢ Document chunks\n‚Ä¢ Metadata"]
    end
    
    TEXT --> EMBED
    EMBED --> VECTOR
    VECTOR --> IVF
    VECTOR --> HNSW
    VECTOR --> FLAT
    
    QUERY --> SEARCH
    IVF --> SEARCH
    HNSW --> SEARCH
    FLAT --> SEARCH
    SEARCH --> RESULTS
```

### Data Flow Architecture

```mermaid
flowchart TB
    subgraph "Data Ingestion"
        UPLOAD["üì§ File Upload\n‚Ä¢ PDF, DOCX, TXT\n‚Ä¢ Size validation\n‚Ä¢ Type checking"]
        EXTRACT["üìú Text Extraction\n‚Ä¢ PyPDF2\n‚Ä¢ python-docx\n‚Ä¢ BeautifulSoup"]
        CHUNK["‚úÇÔ∏è Text Chunking\n‚Ä¢ RecursiveCharacterTextSplitter\n‚Ä¢ 1000 char chunks\n‚Ä¢ 200 char overlap"]
    end
    
    subgraph "Vector Processing"
        EMBED["üî¢ Embedding Generation\n‚Ä¢ SentenceTransformers\n‚Ä¢ Batch processing\n‚Ä¢ Normalization"]
        STORE["üíæ Vector Storage\n‚Ä¢ Milvus collection\n‚Ä¢ Metadata indexing\n‚Ä¢ Automatic flushing"]
    end
    
    subgraph "Query Processing"
        QUERY["‚ùì User Query\n‚Ä¢ Natural language\n‚Ä¢ Intent extraction"]
        SEARCH["üîç Similarity Search\n‚Ä¢ Vector comparison\n‚Ä¢ Top-K retrieval\n‚Ä¢ Score filtering"]
        CONTEXT["üìö Context Assembly\n‚Ä¢ Relevant chunks\n‚Ä¢ Metadata enrichment\n‚Ä¢ Ranking"]
    end
    
    subgraph "Response Generation"
        PROMPT["üìù Prompt Engineering\n‚Ä¢ System instructions\n‚Ä¢ Context injection\n‚Ä¢ Query formatting"]
        LLM["ü§ñ Language Model\n‚Ä¢ OpenAI GPT\n‚Ä¢ Anthropic Claude\n‚Ä¢ Google Gemini"]
        RESPONSE["üí¨ Structured Response\n‚Ä¢ Answer text\n‚Ä¢ Source references\n‚Ä¢ Confidence scores"]
    end
    
    subgraph "Storage Layer"
        MILVUS_DB[("üóÑÔ∏è Milvus\n‚Ä¢ Vector collections\n‚Ä¢ Index management\n‚Ä¢ Query processing")]
        MINIO_DB[("üì¶ MinIO\n‚Ä¢ Original files\n‚Ä¢ Processed documents\n‚Ä¢ Backup storage")]
        ETCD_DB[("‚öôÔ∏è etcd\n‚Ä¢ Metadata\n‚Ä¢ Configuration\n‚Ä¢ Service state")]
    end
    
    UPLOAD --> EXTRACT
    EXTRACT --> CHUNK
    CHUNK --> EMBED
    EMBED --> STORE
    STORE --> MILVUS_DB
    
    QUERY --> SEARCH
    SEARCH --> MILVUS_DB
    MILVUS_DB --> CONTEXT
    CONTEXT --> PROMPT
    PROMPT --> LLM
    LLM --> RESPONSE
    
    STORE --> MINIO_DB
    MILVUS_DB --> ETCD_DB
```

### Performance Optimization Strategy

```mermaid
graph TB
    subgraph "Query Optimization"
        CACHE["üöÄ Response Caching\n‚Ä¢ Redis integration\n‚Ä¢ Query fingerprinting\n‚Ä¢ TTL management"]
        BATCH["üì¶ Batch Processing\n‚Ä¢ Multiple queries\n‚Ä¢ Embedding batching\n‚Ä¢ Connection pooling"]
        FILTER["üéØ Early Filtering\n‚Ä¢ Metadata filters\n‚Ä¢ Score thresholds\n‚Ä¢ Result limiting"]
    end
    
    subgraph "Index Optimization"
        TUNE["‚öôÔ∏è Index Tuning\n‚Ä¢ nlist optimization\n‚Ä¢ nprobe adjustment\n‚Ä¢ Memory allocation"]
        PART["üìÇ Collection Partitioning\n‚Ä¢ Time-based splits\n‚Ä¢ Category grouping\n‚Ä¢ Load balancing"]
        COMP["üóúÔ∏è Vector Compression\n‚Ä¢ Quantization\n‚Ä¢ Dimensionality reduction\n‚Ä¢ Storage optimization"]
    end
    
    subgraph "System Optimization"
        SCALE["üìà Horizontal Scaling\n‚Ä¢ Multiple Milvus nodes\n‚Ä¢ API replicas\n‚Ä¢ Load distribution"]
        MONITOR["üìä Performance Monitoring\n‚Ä¢ Query latency\n‚Ä¢ Throughput metrics\n‚Ä¢ Resource utilization"]
        AUTO["üîÑ Auto-scaling\n‚Ä¢ CPU-based scaling\n‚Ä¢ Memory thresholds\n‚Ä¢ Queue length triggers"]
    end
    
    CACHE -.-> TUNE
    BATCH -.-> PART
    FILTER -.-> COMP
    
    TUNE --> SCALE
    PART --> MONITOR
    COMP --> AUTO
```

## üöÄ Quick Start

### Prerequisites

- **Python 3.10+**
- **Docker & Docker Compose**
- **Git**

### 1. Clone & Setup

```bash
git clone https://github.com/yourusername/genai-boilerplate-milvus-python.git
cd genai-boilerplate-milvus-python
```

### 2. Environment Configuration

```bash
# Copy environment template
cp backend/.env.example backend/.env

# Edit configuration (add your API keys)
nano backend/.env
```

**Required Configuration:**
```bash
# Security
SECRET_KEY="your-super-secret-key-change-in-production"

# AI Provider (at least one)
OPENAI_API_KEY="sk-your-openai-key"
# ANTHROPIC_API_KEY="claude-your-key"
# GOOGLE_API_KEY="your-google-key"

# Milvus (defaults work with Docker)
MILVUS_HOST="localhost"
MILVUS_PORT=19530
MILVUS_COLLECTION_NAME="documents"

# Embedding Settings
EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2"
EMBEDDING_DIMENSION=384
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

### 3. Docker Deployment

```bash
# Start all services (Milvus + API)
docker-compose up --build -d

# Check service status
docker-compose ps
```

**Services Started:**
- **Milvus Standalone**: `http://localhost:19530` (Vector Database)
- **RAG API**: `http://localhost:8000` (FastAPI Backend)
- **Milvus Admin (Attu)**: `http://localhost:3001` (Optional Web UI)
- **MinIO**: `http://localhost:9001` (Object Storage)

### 4. Verify Installation

```bash
# Health check
curl http://localhost:8000/api/v1/health

# API documentation
open http://localhost:8000/docs
```

### 5. Run Example

```bash
# Install dependencies locally (for examples)
cd backend
pip install -r requirements.txt

# Run the comprehensive example
python examples/rag_example.py

# Test all API endpoints
python examples/api_test.py
```

## üìö API Documentation

### üè• Health & Config

```bash
GET  /api/v1/health          # Service health check
GET  /api/v1/config          # Current configuration
```

### üìÑ Document Management

```bash
POST /api/v1/documents/upload         # Upload & process document
GET  /api/v1/documents/stats          # Collection statistics
DELETE /api/v1/documents/{id}         # Delete document
```

### üîç Search & RAG

```bash
POST /api/v1/documents/search         # Semantic search
POST /api/v1/chat/completions         # RAG question answering
```

### Example Requests

#### Document Upload
```bash
curl -X POST "http://localhost:8000/api/v1/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your-document.pdf"
```

#### Document Search
```bash
curl -X POST "http://localhost:8000/api/v1/documents/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the main benefits of AI?",
    "top_k": 5,
    "score_threshold": 0.1
  }'
```

#### Question Answering
```bash
curl -X POST "http://localhost:8000/api/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How does machine learning work?",
    "top_k": 3,
    "ai_provider": "openai",
    "ai_model": "gpt-3.5-turbo"
  }'
```

## üîß Development Setup

### Local Development

```bash
# Backend setup
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Start Milvus only
docker-compose up milvus etcd minio -d

# Run API locally
python src/main.py
```

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `SECRET_KEY` | JWT secret key | Required |
| `MILVUS_HOST` | Milvus server host | `localhost` |
| `MILVUS_PORT` | Milvus server port | `19530` |
| `MILVUS_COLLECTION_NAME` | Collection name | `documents` |
| `EMBEDDING_MODEL` | Sentence transformer model | `all-MiniLM-L6-v2` |
| `EMBEDDING_DIMENSION` | Embedding vector size | `384` |
| `CHUNK_SIZE` | Text chunk size | `1000` |
| `CHUNK_OVERLAP` | Chunk overlap size | `200` |
| `OPENAI_API_KEY` | OpenAI API key | Optional |
| `ANTHROPIC_API_KEY` | Anthropic API key | Optional |
| `GOOGLE_API_KEY` | Google AI API key | Optional |

## üìñ Usage Examples

### Python SDK Usage

```python
import asyncio
from backend.src.services.rag_service import rag_service
from backend.src.core.milvus_client import milvus_client

async def example():
    # Connect to Milvus
    await milvus_client.connect()
    
    # Process a document
    document_id = await rag_service.process_and_store_document(
        file_path="example.pdf",
        metadata={"title": "Example Document"}
    )
    
    # Search for similar content
    results = await rag_service.search_documents(
        query="What is artificial intelligence?",
        top_k=5
    )
    
    # Answer questions using RAG
    response = await rag_service.answer_question(
        question="How does AI work?",
        ai_provider="openai",
        ai_model="gpt-3.5-turbo"
    )
    
    print(f"Answer: {response['answer']}")
    
    # Cleanup
    await milvus_client.disconnect()

# Run example
asyncio.run(example())
```

### Custom LangChain Integration

```python
from backend.src.services.rag_service import rag_service

# Create custom retrieval chain
chain = await rag_service.create_custom_chain(
    chain_type="retrieval_qa",
    ai_provider="openai",
    ai_model="gpt-4"
)

# Use the chain
result = chain({"query": "What are the benefits of RAG?"})
print(result["result"])
```

### Direct Milvus Operations

```python
from backend.src.core.milvus_client import milvus_client

# Connect
await milvus_client.connect()

# Insert documents
documents = [
    {
        "document_id": "doc1",
        "chunk_index": 0,
        "text": "This is example text",
        "metadata": {"source": "example.txt"}
    }
]
chunk_ids = await milvus_client.insert_documents(documents)

# Search similar vectors
results = await milvus_client.search_similar(
    query_text="example query",
    top_k=5,
    score_threshold=0.1
)
```

## üõ†Ô∏è Advanced Configuration

### Custom Embedding Models

```python
# In your .env file
EMBEDDING_MODEL="sentence-transformers/all-mpnet-base-v2"
EMBEDDING_DIMENSION=768
```

**Supported Models:**
- `all-MiniLM-L6-v2` (384 dimensions) - Fast, lightweight
- `all-mpnet-base-v2` (768 dimensions) - High quality
- `all-MiniLM-L12-v2` (384 dimensions) - Balanced
- Any Sentence-Transformers model

### Milvus Configuration

```yaml
# docker-compose.yml customization
milvus:
  environment:
    - MILVUS_CONFIG_PATH=/milvus/configs/milvus.yaml
  volumes:
    - ./config/milvus.yaml:/milvus/configs/milvus.yaml
```

### AI Provider Configuration

```python
# Custom AI provider integration
from langchain.llms.base import LLM

class CustomLLM(LLM):
    def _call(self, prompt: str, stop=None) -> str:
        # Your custom implementation
        return "Custom response"

# Use in RAG service
rag_service._get_llm = lambda provider, model: CustomLLM()
```

## üìä Monitoring & Logging

### Health Checks

```bash
# Application health
curl http://localhost:8000/api/v1/health

# Milvus health
curl http://localhost:9091/healthz

# Collection stats
curl http://localhost:8000/api/v1/documents/stats
```

### Logging Configuration

```python
# Custom logging setup
from loguru import logger

logger.add(
    "logs/rag_{time:YYYY-MM-DD}.log",
    rotation="1 day",
    retention="1 month",
    level="INFO"
)
```

### Performance Monitoring

```bash
# View API metrics
curl http://localhost:8000/metrics

# Monitor Milvus performance
docker logs milvus-standalone --tail 100

# Resource usage
docker stats
```

## üß™ Testing

### Run Test Suite

```bash
# Unit tests
cd backend
pytest tests/

# API integration tests
python examples/api_test.py

# Load testing
pip install locust
locust -f tests/load_test.py --host=http://localhost:8000
```

### Custom Test Examples

```python
import pytest
from backend.src.services.rag_service import rag_service

@pytest.mark.asyncio
async def test_document_processing():
    document_id = await rag_service.process_and_store_document(
        file_path="test_document.txt"
    )
    assert document_id is not None
    
    # Test search
    results = await rag_service.search_documents(
        query="test query",
        top_k=1
    )
    assert len(results) > 0
```

## üöÄ Production Deployment

### Docker Production

```bash
# Production build
docker-compose -f docker-compose.prod.yml up --build -d

# SSL with Nginx
docker-compose -f docker-compose.nginx.yml up -d
```

### Kubernetes Deployment

```yaml
# k8s/milvus-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus
spec:
  replicas: 3
  selector:
    matchLabels:
      app: milvus
  template:
    spec:
      containers:
      - name: milvus
        image: milvusdb/milvus:v2.3.4
        ports:
        - containerPort: 19530
```

### Scaling Considerations

- **Milvus Clustering**: Use multiple Milvus instances
- **API Scaling**: Deploy multiple FastAPI replicas
- **Load Balancing**: Use Nginx or cloud load balancers
- **Database Sharding**: Distribute collections across nodes
- **Caching**: Implement Redis for frequent queries

## üîí Security

### API Security

```python
# Add authentication middleware
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer

security = HTTPBearer()

async def verify_token(token: str = Depends(security)):
    # Implement JWT verification
    if not verify_jwt_token(token.credentials):
        raise HTTPException(status_code=401)
    return token
```

### Network Security

```yaml
# docker-compose with network isolation
networks:
  rag-network:
    driver: bridge
    internal: true
  
services:
  milvus:
    networks:
      - rag-network
```

## ü§ù Contributing

### Development Workflow

```bash
# Fork and clone
git clone https://github.com/yourusername/genai-boilerplate-milvus-python.git
cd genai-boilerplate-milvus-python

# Create feature branch
git checkout -b feature/your-feature-name

# Make changes and test
python examples/rag_example.py
python examples/api_test.py

# Submit PR
git commit -am "Add your feature"
git push origin feature/your-feature-name
```

### Code Standards

- **Python**: Black formatting, type hints, docstrings
- **API**: OpenAPI/Swagger documentation
- **Testing**: Pytest with async support
- **Logging**: Structured logging with loguru
- **Error Handling**: Consistent error responses

## üìÑ File Structure

```
genai-boilerplate-milvus-python/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ endpoints.py          # FastAPI routes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ milvus_client.py      # Vector database client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api_models.py         # Pydantic models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_service.py        # LangChain RAG logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py                   # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py               # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_example.py            # Usage examples
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api_test.py               # API testing
‚îÇ   ‚îú‚îÄ‚îÄ tests/                        # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                    # Container build
‚îÇ   ‚îî‚îÄ‚îÄ .env.example                  # Environment template
‚îú‚îÄ‚îÄ docker-compose.yml                # Development setup
‚îú‚îÄ‚îÄ docker-compose.prod.yml           # Production setup
‚îî‚îÄ‚îÄ README.md                         # This file
```

## üéØ Roadmap

### Current Features
- ‚úÖ FastAPI REST API
- ‚úÖ Milvus vector database integration
- ‚úÖ LangChain RAG pipeline
- ‚úÖ Multi-format document processing
- ‚úÖ Multiple AI provider support
- ‚úÖ Docker containerization

### Planned Features
- üîÑ WebSocket real-time streaming
- üîÑ Advanced chunking strategies
- üîÑ Multi-modal embeddings (text + images)
- üîÑ GraphQL API support
- üîÑ Kubernetes deployment manifests
- üîÑ Monitoring dashboard
- üîÑ Advanced caching layer
- üîÑ Multi-tenant architecture

## üêõ Troubleshooting

### Common Issues

#### Milvus Connection Failed
```bash
# Check if Milvus is running
docker-compose ps milvus

# View Milvus logs
docker-compose logs milvus

# Restart Milvus
docker-compose restart milvus
```

#### Embedding Model Download Issues
```bash
# Pre-download models
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"

# Use different model
export EMBEDDING_MODEL="sentence-transformers/all-distilroberta-v1"
```

#### Memory Issues
```bash
# Increase Docker memory limit
# Docker Desktop ‚Üí Settings ‚Üí Resources ‚Üí Memory ‚Üí 8GB+

# Monitor memory usage
docker stats
```

## üìû Support

- **üêõ Issues**: [GitHub Issues](https://github.com/yourusername/genai-boilerplate-milvus-python/issues)
- **üí¨ Discussions**: [GitHub Discussions](https://github.com/yourusername/genai-boilerplate-milvus-python/discussions)
- **üìñ Documentation**: [Wiki](https://github.com/yourusername/genai-boilerplate-milvus-python/wiki)
- **üöÄ Releases**: [Release Notes](https://github.com/yourusername/genai-boilerplate-milvus-python/releases)

## üìÑ License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **[LangChain](https://langchain.com)** - RAG framework and document processing
- **[Milvus](https://milvus.io)** - High-performance vector database
- **[FastAPI](https://fastapi.tiangolo.com)** - Modern Python web framework
- **[Sentence Transformers](https://www.sbert.net)** - Embedding models

---

**üöÄ Ready to build powerful RAG applications with cutting-edge vector search!**

*Built with ‚ù§Ô∏è for the AI community*
